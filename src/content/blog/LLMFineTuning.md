---
title: LLM FineTuning
pubDate: 2025-12-02
description: LLM 파인튜닝의 대한 학습
author: KT
tags:
  - LLM
  - FineTuning
  - LoRA
  - QLoRA
---
## 파인튜닝이란?
파인튜닝을 이해하려면, 먼저 LLM이 처음에 어떻게 만들어지는지 이해할 필요가 있다. LLM은 Large Language Model의 약자로 인터넷, 책, 코드, 등 방대한 양의 텍스트 데이터를 모으고 "앞에 단어들이 주어졌을 때, 다음 단어를 예측해봐"라는 문제를 시킨다. 이 행동을 적게는 수십억 많게는 수조번 반복하면서 문법, 상식, 패턴, 등을 자연스럽게 익힌다. 이를 사전학습(Pretraining)이라고 한다. 사전학습은 전체적인 패턴을 이해하지만, 특정 도메인을 학습하지는 않습니다. 특정 회사의 정책이나, 게임 규칙 같이 도메인의 영역으로 들어가면 자세히 알지 못합니다.

파인 튜닝은 이렇게 일반적인 패턴을 학습한 모델을 특정 작업에서 잘하도록 약간의 추가학습을 시키는 것입니다. 즉, 처음부터 다시 학습하는 것이 아니라 이미 학습된 모델(pretrained model)을 가져다가 특정 도메인의 지식을 입히는 과정입니다.

## 파인튜닝의 이점
파인 튜닝의 이점은 크게 4가지가 존재한다.

#### 1. 행동 패턴·말투·성격을 학습시킬 수 있다.
행동패턴·성격·말투는 RAG나 프롬프트로 온전하게 구현이 불가능하다. 반면에 파인 튜닝은 행동 패턴·말투·성격을 모델 가중치로 가져가기 때문에 구현이 가능하다

#### 2. 출력의 일관성 증가
프롬프트 기반으로 출력을 설정하면 상황에 따라 성능이 흔들릴 수 있지만, 파인튜닝은 패턴을 내부적으로 학습하기 때문에 항상 비슷한 스타일·구조로 응답한다.

#### 3. 프롬프트 비용(토큰 비용) 절감
프롬프트로 스타일을 강제시키려고 하면 명령을 구체화 시킬수록 시스템 프롬프트의 토큰이 많아지게 된다. 이는 API 비용의 증가 뿐만 아니라 응답시간도 길어지게 만든다.

#### 4. 생각 구조 학습 가능
프롬프트는 일종의 "지시"이다. 형식을 강용하는 방식인 프롬프트와 다르게 파인튜닝은 모델 내부의 사고 흐름, 논리 구조 자체를 조정한다.

## 파인튜닝의 오해
파인튜닝에 대해 오해할만한 내용들을 정리한다.

#### 1. 파인 튜닝은 새로운 지식을 불어 넣는다.
파인 튜닝은 정확히 말하면 새로운 지식을 불어 넣는다기보다, 특정 도메인에 대한 패턴을 학습시키는 과정이다. 파인 튜닝은 논리 구조, 특정 작업 방식, 말투, 성격 같은 패턴을 학습하는 것이지 새로운 지식들을 활용하고 방대한 문서의 내용을 기억하는 것이 아니다.

#### 2. 파인 튜닝이 RAG보다 좋다.
성능을 올린다든 측면에서 비슷하게 보일 수 있지만, 둘은 아예 목적이 다르다. 파인 튜닝은 앞서 말했듯이 특정 도메인에서의 패턴을 학습한다면, RAG는 사전학습되지 않은 데이터를 활용하여 사실 기반의 정보를 반영시키는 기술이다.

- RAG : 지식의 보충
- FineTuning : 능력의 강화

#### 3. 파인 튜닝은 모델을 기존보다 똑똑하게 만든다.
파인 튜닝은 특정 태스크에서만 능력을 강화하고, 이외의 영역에서는 오히려 성능이 떨어질 수도 있다. 따라서 파인 튜닝은 목적이 명확할 때만 해야한다. 모든걸 파인 튜닝으로 해결하려는 것은 좋지 않다.

## PEFT: Parmeter-Efficient Fine-Tuning
전체 모델을 학습시키지 않고, 필요한 파라미터만 학습해서 훨씬 적은 GPU로, 훨씬 빠르게 파인 튜닝하는 기술이다. 가장 전통적인 파인 튜닝은 Full Fine-Tuning으로 pretrained 모델의 가중치를 초기값으로 가지고, 전체 가중치를 업데이트하는 방식이였다. PEFT는 전체를 학습하지 말고, 필요한 부분만 조금 학습하자로 접근한다. PEFT는 전체 모델 파라미터는 고정(freeze)하고, 작은 어댑터(Adaptor)만 붙여 학습하는 방식이다.

#### 동작원리
1. 원본 모델의 가중치를 전부 동결(freeze)한다.
2. 기술에 따라 다르지만 모델 일부에 작은 어댑터 모듈을 끼워 넣는다.
3. 끼워 넣은 작은 모듈에서만 학습이 이뤄진다.
4. Inference는 원본 + 어댑터가 합쳐져서 동작한다.

#### PEFT 장점
1. 전체 가중치를 학습하지 않기 때문에 비용이 절감된다.
2. 학습 속도가 빠르다.
3. 원본 모델을 손상시키지 않는다.
4. 저장 용량이 작아서 배포가 쉽다.
5. 실험 속도가 빨라 반복에 용이하다.

#### PEFT 단점
1. 지식의 업데이트에는 약하다.
2. 과하게 파인튜닝하면 성능이 저하될 수 있다.
3. 데이터 수가 적거나 데이터 품질이 떨어지면 성능 편향이 생길 수 있다.
4. 모델의 구조 자체를 바꾸지는 못한다.

PEFT 기술은 크게 4가지로 나뉜다.
#### 1. (Soft) Prompt Tuning
프롬프트(Tokens)를 학습하는 방식이다. 사람이 작성하는 프롬프트 문장을 학습하는게 아니라, 프롬프트에 해당하는 가상의 벡터(embedding)을 학습하는 방식이다. 기존에는 프롬프트로 답변을 강제했다면, 매번 이 프롬프트를 붙이지 말고, 프롬프트를 벡터로 만들어서 자동으로 앞에 붙이는 방식으로 학습한다. 예를들어, 
```
너는 오늘부터 아주 정중한 비즈니스 말투로 대답해.
문장은 ~습니다로 끝내.
```
기존에는 위의 프롬프트를 붙여줬었는데, 프롬프트 튜닝을 하면 유저가 입력하면 자동으로 앞에 컴퓨터만 볼 수 있는 벡터를 생성해줍니다.
```
[정중한 말투를 유도하는 20개의 학습된 벡터] + 유저 입력
```
###### 장점
- 파라미터가 매우 적다
- 매우 가볍고 빠르다.

###### 단점
- 생성 능력을 강하게 바꾸기는 힘들다.
- 복잡한 추론이나 큰 변화가 필요한 파인튜닝에는 부적합하다.

###### 언제 쓰면 좋을까?
- 요약
- 문서 분류
- 감정 분석

#### 2. P-Tuning
프롬프트 튜닝의 진화 버전이다. 기존에는 사용자 입력 앞에 임베딩을 붙였지만, 해당 방식은 모델 깊은 곳에 영향은 못주기에 효과가 미미했다. P-Tuning은 Transformer 레이어 안으로 들어가 각 레이어에 벡터를 삽입하는 방식이다. 
```
Q
[Prefix K] + K
[Prefix V] + V
```
학습 가능한 벡터 Prefix K, Prefix V를 삽입하여 Q와 attention 되기 때문에 모델의 사고 흐름을 직접 조종할 수 있게 된다.

###### 장점
- (Soft) Prompt 보다 훨씬 강력하다
- 모델 흐름에 직접 영향을 준다.
- 파라미터가 여전히 적다

###### 단점
- 후에 나올 LoRA보단 약하다
- 말투나 행동같은 패턴 학습에는 부족하다

###### 언제 쓰면 좋을까?
- 긴 문서 요약
- 모델의 사고를 조금 수정할 때

#### 3. LoRA, Low-Rank Adaption
LLM 파인튜닝의 표준이라고도 부를 수 있는 방식이다. 큰 모델의 가중치 전체를 바꾸지 않고 그 가중치 옆에 작은 Low-Rank 행렬을 붙여 학습하는 기법이다. "W를 직접 업데이트 하지않고 옆에 BA를 학습해서 새로운 가중치 **W’ = W + BA** 로 사용하자"가 LoRA의 핵심 아이디어다.

거대한 가중치 행렬의 변화는 낮은 차원에 충분히 담을 수 있었고, W는 냅두고 변화량  **ΔW** 만 낮은 차원에 표현해서 학습하는 방식입니다. 개인적으로 ResNet의 Residual Learning과 유사한 철학인듯 하다.

![LLMFineTuning-20251202-1.png](/images/blog/LLMFineTuning-20251202-1.png)

###### 장점
- 가장 강력한 PEFT 기법이다.
- 말투·행동 패턴 학습에 최적
- 파인 튜닝 효과가 거의 Full Fine-Tuning급이다.
- LoRA 다중 로딩·병합이 가능하다.

###### 단점
- 파라미터가 앞선 방식들보단 조금 더 크다
- 병합이 복잡하고 충돌 위험성이 존재한다.

###### 언제 쓰면 좋을까?
- 강력한 스타일·패턴을 학습할 때
- 성격·전략을 수정하고 싶을 때

#### 4. QLoRA
LoRA의 업그레이드 버전이다. 모델 가중치를 4bit로 압축하고 LoRA 모듈은 FP16/BF16으로 학습 시키는 방식으로 메모리 절약을 극대화 시켰다.

###### 장점
- LoRA보다 메모리 2~3배 절약

###### 단점
- 4bit 양자화로 인해 미세한 표현 능력이 저하될 수 있다.
- rank가 너무 낮아질 경우 underfitting의 위험이 존재한다.

###### 언제 쓰면 좋을까?
- 개인 GPU로 7B~13B 모델 파인 튜닝
- 실전 서비스용 AI

#### RAG와 파인튜닝을 함께 사용해야 하는가?
대부분의 실제 LLM 서비스는 RAG와 파인튜닝을 결합한다. 두 기술이 서로 다른 목적을 수행하며 서로의 단점을 상호 보완해주는 효과가 존재하기 때문이다. 파인 튜닝은 모델의 패턴, 말투, 논리 구조, 태스크 수행 방식을 학습시키는 기술이고, RAG는 외부 문서를 검색하고 참고하여 모델의 지식을 확장 시켜준다.

즉, RAG는 지식을 강화하고, Fine-Tuning은 능력을 강화한다.
