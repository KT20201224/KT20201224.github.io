---
title: Word2Vec 논문 내용 정리
pubDate: 2025-11-05
description: Word2Vec 논문 내용 정리
author: KT
tags:
  - Word2Vec
---
## Word2Vec 이전
기존의 NLP 방식에서는 구현이 쉽고 직관적이라는 이유로 단어를 사전 상의 인덱스로 표현했습니다. 
- "사과" = 15
- "자동차" = 1052
- "바나나" = 237

해당 방식은 단어 간 의미적 관계를 파악하지는 못하고, 단순히 어휘 사전에서의 구별만 가능했습니다. 하지만, 단순한 기법들의 경우에는 많은 분야에서 한계에 도달했습니다. 예를 들어, 자동 음성 인식에서 음성 데이터 수가 수백만 단어 정도로 제한적입니다. 이는 모델의 성능이 데이터의 크기에 의해 결정되는 것입니다. 더이상 단순히 모델을 발전 시키는 것만으로는 의미가 없어졌고, 다른 부분에 더 집중해야 했습니다. 

기계 학습 기법이 발전하면서 큰 데이터셋에서 복잡한 모델을 학습하는 것이 가능해졌습니다. 성능을 위해 여러 아이디어가 제시됩니다. NNLM에서 투영층-은닉층-출력층으로 가는 과정의 연산을 줄이기 위해 이진 트리를 활용한 계층적 소프트맥스를 도입하거나, Collobert & Weston 접근, RNN, 등 여러 방식을 도입하지만 이 방식들의 공통적인 문제가 존재했습니다. 모든 방법이 "은닉층" 때문에 계산 복잡도가 높았습니다. 큰 데이터셋으로 갈 수록 학습시간이 오래걸리는 문제에 봉착합니다.

## Word2Vec의 등장
word2vec은 "은닉층 때문에 시간이 오래걸리는 것 같은데 꼭 필요해?"라는 질문을 던집니다. 기존에는 은닉층의 비선형성이 신경망의 핵심 강점이고, 복잡한 패턴 학습에 필수적이라는 의견이였습니다. word2vec은 "여러 은닉층을 활용하면 적은 데이터로도 모델이 특징들을 잘 이해하지 않을까?" 라는 생각을 뒤집어서 생각합니다.

###### 아니 계산이 너무 많아? 그러면 계산을 줄이고 데이터를 늘려!

기존의 복잡한 모델은 섬세한 패턴을 포착 가능하지만, 학습이 느리고 확장성이 떨어집니다. 하지만, 새로 제시한 방식은 모델이 매우 단순해 학습이 빠르고 확장성이 높습니다. 논문에서 N-gram 모델의 사례를 보여주는데, 개인적으로 word2vec이 나올 수 있었던 이유는, 목표가 달랐습니다. NNLM은 신경망을 통해 다음 단어를 예측하는 것에 집중했다면, word2vec에서는 단어간의 관계를 학습하는 관계파악에 초점을 두고 있습니다. 

word2vec의 접근은 주변 단어로 중심 단어를 예측하거나 그 반대를 수행하는 것입니다. 비슷한 문맥은 비슷한 벡터를 만들게 되고, 이를 통해 의미 관계가 자동으로 학습된다고 생각했습니다.

## 2가지 모델 아키텍처
word2vec에서는 중심 단어로 주변 단어를 예측(Skip-gram)하거나, 주변 단어로 중심 단어를 예측(CBOW)하는 두 구조를 모두 소개합니다.

![Word2Vec-20251105-1.png](/images/blog/Word2Vec-20251105-1.png)
#### CBOW, Continuous Bag-of-Words
CBOW는 주변 단어들로 중간 단어를 예측합니다. 은닉층 없이 출력층, 투영층, 입력층으로 구성됩니다. 은닉층 없이 투영층으로 모든 단어가 투영 행렬을 공유하고 벡터들을 단순 평균합니다. 글로 표현하는 데 한계가 있기는 하지만, 예를 들어보겠습니다.

"나는 오늘 OOO 갔다."

1. 입력층 : 주변 단어들이 one-hot vector로 입력된다. "나는", "오늘", "갔다."
2. 투영층 : 각 단어를 임베딩 행렬에서 찾아서 단어 벡터로 변환한다.
3. 평균/합 연산 : 주변 단어들의 임베딩을 평균 내서 문맥 벡터를 생성
4. 출력층 : 그 문맥 벡터를 통해 타깃 단어가 무엇인지 확률적으로 예측

문법이나 순서는 고려하지 않고, 오로지 단어들의 관계만 학습합니다. 데이터가 많아질수록(자주 등장하는 단어일수록) 임베딩 공간에서의 정확한 표현이 가능합니다.

#### Skip-gram
Skip-gram은 CBOW와 방향만 반대입니다. 희귀 단어를 더 잘 학습할 수 있고, 의미적 관계를 더 잘 포착합니다.

💡 정리
CBOW는 문맥이 주어졌을 때 단어를 이해하는 모델이고, Skip-gram은 단어가 주어졌을 때 문맥(관계)를 이해하는 모델입니다. 

CBOW는 여러 단어들이 등장했을 때, 그 공통된 의미의 중심을 찾습니다.(벡터의 평균을 구함) 즉, 문맥적 의미 공간을 학습합니다.

Skip-gram은 어떤 단어가 등장했을 때, 그 단어 주변에는 어떤 단어들이 같이 나오는지를 학습합니다. 즉, "이 단어는 어떤 환경에서 주로 등장하는가"를 배웁니다. 단어 간의 관계를 풍부하게 포착해, **king - man + woman ≈ queen** 이런 결과가 가능해집니다.
