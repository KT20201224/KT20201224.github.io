---
title: Understanding neural networks through sparse circuits, OpenAI
pubDate: 2025-11-14
description: 희소 회로로 이해하는 신경망
author: KT
tags:
  - OpenAI
  - NeuralNetworks
---
신경망은 가장 강력한 AI 시스템을 구동하지만, 여전히 이해하기가 어렵습니다. 우리는 모델들을 명시적이고 단계별 지침으로 작성하지 않습니다. 단지 작업을 마스터할때까지 수십억개의 내부 연결, "가중치"를 조정하면서 학습합니다. 우리는 내부구조는 알지 못합니다. x -> F(x)를 수많은 데이터들로 찾아갈 뿐이죠.

## 해석 가능성
AI가 강력해지고 과학, 교육, 의료 분야의 의사결정에 실제 영향을 미치면서, 모델의 동작과정을 이해하는 것은 이제 필수적입니다. 해석 가능성은 모델의 출력을 이해하는데 도움을 주는 방법들을 의미합니다. 이는 여러가지 방식이 존재합니다.

예를들어, 추론 모델들은 최종 답변에 도달하는 과정에서 자신의 작업을 설명하도록 유도됩니다. 사고과정 해석 가능성은 이러한 설명을  활용하여 모델의 행동을 모니터링합니다. 이는 즉각적으로 유용하지만, 설명에 완전히 의존하는 것은 취약한 전략입니다.

반면에, 이 연구의 초점인 메커니즘 해석 가능성은 모델 계산을 완전히 역공학하는 것을 추구합니다. 즉각적으로 유용하지는 않지만, 원칙적으로는 모델 행동에 대한 더 완벽한 설명을 제공할 수 있습니다. 우리에게 더 많은 확신을 줄 수 있지만, 복잡한 행동에 대한 설명으로 가는 길은 너무나도 어렵습니다.

해석 가능성은 여러 핵심 목표를 지원합니다. 예를 들어, 더 나은 감독을 가능하게 하고 안전하지 않거나 전략적으로 잘못 정렬된 행동의 조기 경고 신호를 제공합니다.

이 연구에서 우리는 모델을 해석하기 더 쉽게 만드는 방식으로 훈련시킬 수 있음을 보여줍니다. 우리의 작업을 조밀한 네트워크에 대한 사후 분석의 유망한 보완책으로 봅니다.

우리의 연구로부터 가장 강력한 모델들의 복잡한 행동을 완전히 이해하기까지는 긴 여정이 있습니다. 그럼에도 불구하고, 단순한 행동들에 대해서는, 우리의 방법으로 훈련된 희소 모델들이 이해 가능하고 해당 행동을 수행하기에 충분한 작고 얽히지 않은 회로들을 포함하고 있음을 발견했습니다.



## 희소 모델 학습
이전의 메커니즘 해석 가능성 연구는 조밀하고 복잡한 네트워크에서 시작하여, 이를 풀어내려고 시도했습니다. 이러한 네트워크에서 각각의 뉴런은 수천개의 다른 뉴런들과 연결되어 있습니다. 대부분의 뉴런들은 여러가지 서로 다른 기능을 수행하는 것처럼 보이며, 이는 이해하기가 거의 불가능해 보이게 만듭니다.

우리가 얽히지 않은 신경망을 훈련시킨다면 어떨까요? 훨씬 더 많은 뉴런을 가지지만, 각 뉴런이 단지 수십개의 연결만 기자도록 말이죠. 그렇다면 아마도 결과적인 네트워큰느 더 단순하고, 이해하기 쉽지 않을까요??

이 원칙을 염두에 두고, 우리는 GPT-2와 같은 기존 언어 모델들과 매우 유사한 아키텍처를 가진 언어 모델을 훈련시켰지만, 한가지 작은 수정을 가했습니다. 모델 가중치의 대다수를 0이되도록 강제했습니다. 즉 연결을 죽인 것이죠. 간단한 변경이지만, 이것이 모델의 내부 계산을 실질적으로 풀어준다고 주장합니다.