---
title: 랭체인의 전체적인 흐름
pubDate: 2025-12-09
description: 랭체인에서 이루어지는 과정들을 간단하게 정리
author: KT
tags:
  - LangChain
  - Prompt
  - Embedding
  - VectorStore
---
#### 1. 데이터 로더
여러 raw data(pdf, Markdown, web Page)를 한 곳에 모아 LangChain이 쓸 수 있는 Document 객체 리스트로 만든다. LangChain은 다양한 로더들이 존재하기 때문에 어떤 형식의 문서도 Document화 시킬 수 있다.
1. PDF 로더 : pdf의 경우에는 각 페이지가 한 document로 로드된다.
2. HWP 로더 : 문서 로드 자체는 이루어지지만, 일부 기능은 지원되지 않는다.
3. CSV 로더 : 각 행마다 document 객체로 반환
4. DataFrame 로더 
외에도 정말 다양한 데이터 로더들이 존재하기 때문에, 상황에 맞게 데이터 로더를 잘 사용해야 한다.

Document는 `page_content` + `metadata`로 이루어지고, RAG 품질에 큰 영향을 주기 때문에 처음 document 객체 로드 단계에서 같이 설계하는 것이 좋다.


#### 텍스트 분할
데이터 로드 단계에서 다양한 raw data를 받아와서 document 객체로 통일 시켰다면, 텍스트 분할은 document로 된 긴 텍스트들을 적당한 크기의 청크(chunks)로  잘게 나누는 단계입니다. 페이지 전체를 넣어도 좋지만, 정말 필요한 정보만 선별해서 LLM에 넣는 방식이 더 높은 답변 품질을 제공하게 될 확률이 높습니다. 문서를 분할하는 전략은 문단, 문장, 글자 수, 토큰 수 등 다양한 전략이 존재하며 정답은 없습니다.

필요에 따라 소스 타입별로 spliter를 다르게 적용할 수 있지만, 보통 `RecursiveCharacterTextSplitter`로 기본 splitter를 구성해도 좋은 성능이 나온다고 합니다.

또한 chunk 단위로 나눈다고 하더라도, 원본 document의 메타데이터가 따라오기 때문에 출처를 역으로 trace도 가능하다고 합니다.

###### 💡Semantic chunker
토큰, 문자 단위로 나누는 분할들과 다르게 의미적 유사성을 기준으로 청크를 분리하는 방식입니다. ![LangChainFlow-20251209-1.png](/images/blog/LangChainFlow-20251209-1.png)유사한문장들 간 거리를 구하고, 계산된 거리 값들 중에서 텍스트를 나누는 기준인 breakpoint를 기준으로 문장을 분리하여 청크로 반환하는 방식입니다.

#### 임베딩
앞에서 나눈 청크들을 숫자 표현인 벡터로 변환하는 과정입니다. LLM은 텍스트를 온전히 받아들이지 못하기 때문에 모든 것을 벡터화해서 읽습니다. 임베딩 알고리즘마다 생성하는 벡터의 차원이 서로 다릅니다. 차원이 높아질 수록 벡터 표현이 정교해지지만, 차원이 성능과 비례하지는 않습니다.

클라우드 기반 API로 제공되는 OpenAI 임베딩은 다국어 지원이 되고, 성능이 준수하지만, 처리하려는 데이터 양에 따라 비용이 증가합니다. 무료인 HuggingFace 임베딩 모델들은 로컬에서 실행하게 되면 시간이 많이 소요될 수 있습니다. 따라서 임베딩 작업을 수행하고 결과물을 vecotrstore나 DB에 저장하는 방법도 고려해 볼만하다.

#### 벡터 스토어
벡터 스토어를 사용하면 semantic search를 통해 관련 정보를 빠르게 조회할 수 있습니다. 로컬 방식은 비용과 보안 측면에서 유리하지만 확장성이 제한적이고, 클라우드 방식은 확장성이 뛰어나지만 비용적 부담을 무시할 수 없습니다. 벡터스토어는 크게 3가지가 존재합니다.
- Chroma : 로컬, 파일 기반의 벡터DB이다. 멀티모달을 지원한다.
- FAISS : 메타에서 만든 라이브러리로 효율적인 유사도 검색과 클러스터링을 위한 라이브러이이다.
- 클라우드형 : Pinecone, Qdrant, Weaviate 등

#### 리트리버
지금까지는 데이터를 저장하는 과정이였다면, 이제부터는 지금까지 다듬어서 만든 청크들을 저장한 벡터DB에서 관련성 높은 청크들로 찾아오는 단계입니다. 질문으로 들어온 텍스트를 LLM은 역시나 벡터로 변환해서 벡터DB의 데이터들과 유사성을 계산합니다. 코사인 유사성과 MMR 같은 수학적 방법을 사용합니다
- 코사인 유사성 : 코사인 유사도는 두 벡터의 각도를 기반으로 유사도를 측정하는 방식이다. 수학 공식까지는 필요없고, 의미가 가까울 수록(각도가 좁을 수록) 유사도는 1에 가까워지고, 의미가 멀어지면 유사도는 0에 가까워진다.
- MMR(Max Marginal Relevance) : 코사인 유사도로만 검색하면 유사한 문장간의 의미가 과하게 겹쳐서 여러 청크를 가져온 의미가 없어질 수 있다. 이를 방지하기 위해 나온 방식으로 유사성 뿐 아니라 그 안에서 서로 다른 내용(diversity)인지까지 고려하는 방식이다.