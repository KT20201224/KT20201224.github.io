---
title: 활성화 함수
pubDate: 2025-10-21
description: 활성화 함수에 대한 공부
author: KT
tags:
  - ActivationFunction
  - StepFunction
  - ReLU
  - Sigmoid
  - Tanh
---
활성화 함수는 MLP나 인공 신경망에서 뉴런이 얼마나 활성화 될지를 결정하는 "비선형 함수"이다. 단순한 선형 모델을 비선형 문제로 확장시켜주는 핵심 역할을 합니다.

#### 왜 활성화 함수가 필요한가?
활성화 함수가 없으면 네트워크는 결국 단순하게
y = W_n W_{n-1} … W_1 x + b
와 같은 선형 변환으로 축약됩니다. 아무리 Layer를 쌓아도 선형 관계만 표현하게 되는 문제가 생깁니다. 따라서, 비선형 함수인 활성화 함수를 입혀야 복잡한 패턴(이미지, 자연어, 등)을 학습할 수 있게 됩니다. 아래는 다양한 활성화 함수들을 소개합니다.

#### Step Function
![Activation Function-20251021-1.png](/images/blog/Activation%20Function-20251021-1.png)
Step Function은 신경망의 기본 모델인 **퍼셉트론(Perceptron)**에서 사용되는 가장 기본적인 활성화 함수입니다. 특정 임계값(threshold)을 기준으로 0 또는 1을 출력하는 불연속 함수입니다. 출력이 두가지로 제한 되어, Yes/No 나 True/False 와 같은 이진 분류에 적합합니다.

###### 장점
- 출력이 명확하게 0 또는 1로 구분되어 해석이 쉽습니다.
- 단순 비교 연산만을 요구하므로 계산이 매우 빠릅니다.

###### 단점
- 임계값에서 불연속하므로, 미분이 불가능합니다. 이는 역전-파(Backpropagation) 알고리즘을 사용할 수 없다는 뜻이기도 합니다.
- 임계점이 아닌 지점 역시 gradient가 0이므로 학습이 불가능해 현대 딥러닝에선 사용하기 힘듭니다.

#### Sigmoid
![Activation Function-20251021-2.png](/images/blog/Activation%20Function-20251021-2.png)
Sigmoid는 S자 곡선 형태의 부드러운 활성화 함수로, StepFunction의 미분 불가능 문제를 해결하기 위해 개발된 대표적인 함수입니다. 모든 지점에서 연속이고, 미분가능합니다. 또한, 그래프의 양 끝이 0과 1에 수렴하여 확률적으로 해석이 가능해집니다.

###### 장점
- 모든 실수 구간에서 미분 가능하므로, 역전파 알고리즘에 사용이 가능하여 신경망 학습이 가능합니다. 
- 출력값이 0~1 사이의 값을 가지므로 확률적으로 해석이 가능해집니다.

###### 단점
- 입력값이 |x| > 5 일때, gradient가 0에 가까워져 학습이 느려지거나 멈추게 되는데, 이는 Layer가 깊어지게 되면 치명적인 문제가 됩니다. 
- 시그모이드는 항상 양수만 출력하므로, 모든 가중치가 한방향으로만 조정됩니다. all 가중치 증가/all 가중치 감소. 이를 zero-centered문제라고 합니다.
- e^-x 연산이 시간이 오래 걸린다는 단점이 존재합니다. 이때문에, 모바일/임베디드 환경에서는 Hard Sigmoid를 사용하거나,ReLU를 선호합니다.

#### Tanh 활성화 함수
![Activation Function-20251021-3.png](/images/blog/Activation%20Function-20251021-3.png)
Tanh(Hyperbolic Tangent)는 시그모이드의 zero-centered 문제를 해결한 S자 곡선 형태의 활성화 함수입니다. 출력범위가 -1~1이라는 것을 제외하고는 시그모이드 함수와 동일합니다.

###### 장점
- zero-centered 문제를 해결하여 가중치가 독립적으로 증가/감소가 가능합니다.
- 시그모이드는 최대 미분값이 0.25인 반면 Tanh는 최대 기울기가 1로, 학습 초기에 더 빠른 수렴이 가능합니다.

###### 단점
- 입력값이 |x| > 3에서 여전히 그래디언트 소실 문제가 존재합니다.
- 시그모이드 함수보다 수식이 복잡해 연산속도가 더 느립니다.

#### ReLU
![Activation Function-20251021-4.png](/images/blog/Activation%20Function-20251021-4.png)
ReLU(Rectified Linear Unit)는 현대 딥러닝의 표준 활성화 함수로, Sigmoid와 Tanh의 그래디언트 소실 문제를 획기적으로 해결했습니다. 모양은 단순합니다. 음수는 0으로, 양수는 그대로 통과시킵니다. 엄밀히 0에서는 미분이 불가능하지만, 미분값을 0 또는 1로 정의합니다. 단순하지만 비선형 함수이고, 