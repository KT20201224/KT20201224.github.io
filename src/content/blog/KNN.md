---
title: K-Nearest Neighbors, KNN
pubDate: 2025-10-14
description: KNN 알고리즘 학습, 실습
author: KT
tags:
  - KNN
  - ML
---
## K-Nearest Neighbors 알고리즘 
KNN의 핵심아이디어는 "비슷한 것들은 가까이 있다."입니다. 새로운 데이터 포인트가 주어졌을 때, 그 주변의 가장 가까운 K개의 이웃들의 정보를 바탕으로 정답을 예측합니다.

---
## KNN 작동원리
#### 1. 학습 단계
KNN은 학습이라는 과정이 실제로 존재하지는 않습니다. 가중치를 학습하거나 모델 파라미터를 최적화하는과정이 없고, 데이터를 메모리에 저장만 합니다.

#### 2. 예측 단계
실제 계산은 예측 단계에서 일어납니다.
1. 거리계산 : 새로운 데이터 포인트와 학습 데이터 간의 거리를 계산합니다.
2. 정렬 : 계산된 거리를 오름차순으로 정렬합니다.
3. 이웃 선택 : 가장 가까운 K개의 데이터 포인트 선택
4. 예측 :
	- 분류 : K개의 이웃 중 가장 많이 나타난 클래스 선택
	- 회귀 : K개의 이웃의 평균 값 계산

---
## 거리 측정 방법
#### 1. 유클리드 거리
우리가 흔히 알고 있는 두 점 사이의 직선거리이다. 가장 직관적이고, 연속형 데이터에 적합하다.
$$
d = √[(x₁-x₂)² + (y₁-y₂)² + ... + (zₙ-zₙ)²]
$$
#### 2. 멘헤튼 거리
격자 형태로 이동하는 거리, 2차원 상으로 볼 때, 점과 점 사이의 가로+세로 길이의 합이다
$$
d = |x₁-x₂| + |y₁-y₂| + ... + |zₙ-zₙ|
$$
---
## K 값 선택
K는 KNN에서 가장 중요한 하이퍼파라미터이다. K값을 어떻게 설정하느냐에 따라 장단점이 존재합니다.
#### 1. K가 너무 작을 때
- 장점
	- 데이터의 세밀한 패턴을 포착 가능하다.

- 단점
	- 노이즈에 매우 민감하다.
	- 결정하는 경계가 복잡하고 불안정하다.
	- 아상치 영향을 많이 받는다.

#### 2. K가 너무 클 때
- 장점
	- 안정적이고, 노이즈에 강하다

- 단점
	- 지역적 패턴을 무시하는 경향이 있다.